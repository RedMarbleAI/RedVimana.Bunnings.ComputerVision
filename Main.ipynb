{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install opencv-python\n",
    "!{sys.executable} -m pip install Pillow\n",
    "!{sys.executable} -m pip install \"tensorflow<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BoundingBox import *\n",
    "from Camera import *\n",
    "from CameraConfiguration import *\n",
    "from Configuration import *\n",
    "from CoordinateConverter import TopDownCoordinateConverter, BarycentricCoordinateConverter\n",
    "from FieldState import *\n",
    "from HumanDetector import *\n",
    "from Point2 import *\n",
    "from Size2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = Configuration()\n",
    "configuration.ModelPath = \"Models/ssd_mobilenet_v1_coco_2018_01_28/frozen_inference_graph.pb\"\n",
    "configuration.ModelImageSize = Size2(1920, 1080)\n",
    "# configuration.Cameras = [CameraConfiguration(\"rtsp://172.20.10.6:8080\")]\n",
    "# configuration.Cameras = [RTSPCameraConfiguration(\"rtsp://wowzaec2demo.streamlock.net/vod/mp4:BigBuckBunny_115k.mov\", TopDownCoordinateConverter(Point2(0, 0), Size2(1870, 1465)))]\n",
    "configuration.Cameras = [FileCameraConfiguration(\"BunningsTest10-3.mp4\", BarycentricCoordinateConverter([Point2(0, 265), Point2(1392, 454), Point2(1193, 732), Point2(419, 1076)], [Point2(1050, 655), Point2(1483, 651), Point2(1477, 805), Point2(1192, 807)]))]\n",
    "cameras = [cameraConfiguration.CreateCamera() for cameraConfiguration in configuration.Cameras]\n",
    "# humanDetector = COCOTensorflowHumanDetector(modelPath=configuration.ModelPath, threshold=0.15)\n",
    "\n",
    "yoloModelConfig = \"Models/yolov3/yolov3.cfg\"\n",
    "yoloModelWeights = \"Models/yolov3/yolov3.weights\"\n",
    "yoloModelClasses = \"Models/yolov3/coco.names\"\n",
    "humanDetector = YoloV3OpenCVDetector(yoloModelConfig, yoloModelWeights, yoloModelClasses, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "\n",
    "fieldStateHistory = []\n",
    "\n",
    "while True:\n",
    "    fieldState = FieldState()\n",
    "    fieldState.Time = time.time()\n",
    "    hasImages = False\n",
    "    \n",
    "    for camera in cameras:\n",
    "        image = camera.GetFrame()\n",
    "        fieldState.Images.append(image)\n",
    "        if image is not None:\n",
    "            hasImages = True\n",
    "            image = cv2.resize(image, (configuration.ModelImageSize.Width, configuration.ModelImageSize.Height))\n",
    "            # image = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "            boxes = humanDetector.Detect(image)\n",
    "\n",
    "            for i in range(len(boxes)):\n",
    "                box = boxes[i]\n",
    "                globalPoint = camera.Configuration.ConvertToGlobalCoordinates(configuration.ModelImageSize, box.Center)\n",
    "                if (globalPoint is not None):\n",
    "                    fieldState.People.append(globalPoint)\n",
    "                    cv2.rectangle(image, (box.TopLeft.X, box.TopLeft.Y), (box.BottomRight.X, box.BottomRight.Y), (255, 0, 0), 2)\n",
    "                    # print(len(fieldStateHistory))\n",
    "                    \n",
    "            # matplotlib.pyplot.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            # matplotlib.pyplot.show()\n",
    "            cv2.imshow(\"Live View\", image)\n",
    "            cv2.waitKey(1)\n",
    "            floorPlanImage = fieldState.DrawImage()\n",
    "            floorPlanImage = numpy.array(floorPlanImage)\n",
    "            # floorPlanImage = floorPlanImage[:, :, ::-1].copy() \n",
    "            cv2.imshow(\"Floor Plan\", floorPlanImage)\n",
    "            cv2.waitKey(1)\n",
    "        \n",
    "    \n",
    "    if hasImages:\n",
    "        fieldStateHistory.append(fieldState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modification for writing it out\n",
    "\n",
    "import matplotlib.pyplot\n",
    "\n",
    "fieldStateHistory = []\n",
    "\n",
    "framesWritten = 0\n",
    "\n",
    "while True:\n",
    "    fieldState = FieldState()\n",
    "    fieldState.Time = time.time()\n",
    "    hasImages = False\n",
    "    \n",
    "    for camera in cameras:\n",
    "        image = camera.GetFrame()\n",
    "        \n",
    "        fieldState.Images.append(image)\n",
    "        if image is not None:\n",
    "            hasImages = True\n",
    "            image = cv2.resize(image, (configuration.ModelImageSize.Width, configuration.ModelImageSize.Height))\n",
    "\n",
    "            boxes = humanDetector.Detect(image)\n",
    "\n",
    "            for i in range(len(boxes)):\n",
    "                box = boxes[i]\n",
    "                try:\n",
    "                    globalPoint = camera.Configuration.ConvertToGlobalCoordinates(configuration.ModelImageSize, box.Center)\n",
    "                    if (globalPoint is not None):\n",
    "                        fieldState.People.append(globalPoint)\n",
    "                        cv2.rectangle(image, (box.TopLeft.X, box.TopLeft.Y), (box.BottomRight.X, box.BottomRight.Y), (255, 0, 0), 2)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            floorPlanImage = fieldState.DrawImage()\n",
    "            floorPlanImage = numpy.array(floorPlanImage)\n",
    "            \n",
    "            cv2.imwrite(\"Output/Video/\" + str(framesWritten).zfill(7) + \".jpg\", image)\n",
    "            cv2.imwrite(\"Output/Floor Plan/\" + str(framesWritten).zfill(7) + \".jpg\", floorPlanImage)\n",
    "            \n",
    "            framesWritten += 1\n",
    "    else:\n",
    "        continue\n",
    "    break\n",
    "    \n",
    "    if hasImages:\n",
    "        fieldStateHistory.append(fieldState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these FFPMEG commands to stitch the images into a video\n",
    "# ffmpeg -framerate 24 -i C:\\Users\\ongan\\Documents\\RedVimana.Bunnings.ComputerVision\\Output\\Video\\%07d.jpg Video.mp4\n",
    "# ffmpeg -framerate 24 -i \"C:\\Users\\ongan\\Documents\\RedVimana.Bunnings.ComputerVision\\Output\\Floor Plan\\%07d.jpg\" -vf \"pad=ceil(iw/2)*2:ceil(ih/2)*2\" \"Floor Plan.mp4\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
